{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07fa29b",
   "metadata": {},
   "source": [
    "# Steps to generate API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2dd947",
   "metadata": {},
   "source": [
    "To get a ChatGPT API key, you need to create an account on the OpenAI website and then generate a key. Here are the steps involved:<br>\n",
    "\n",
    "Go to the OpenAI website and click on the \"Sign up\" button.<br>\n",
    "Enter your email address and password, or sign up using your Google or Microsoft account.<br>\n",
    "Verify your email address.<br>\n",
    "Click on API.<br>\n",
    "Click on your profile picture in the top right corner of the screen.<br>\n",
    "Click on View API keys.<br>\n",
    "Click on the \"Create new secret key\" button.<br>\n",
    "Copy your API key and keep it safe. You will need it to access the ChatGPT API.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a36c45",
   "metadata": {},
   "source": [
    "# Setting up environment and API Key to use chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a139197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import os\n",
    "import PyPDF2\n",
    "import time\n",
    "from fpdf import FPDF\n",
    "\n",
    "openai.api_key = '<<API Key>>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfc538",
   "metadata": {},
   "source": [
    "# OpenAI Model Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394dceb3",
   "metadata": {},
   "source": [
    "# `get_completion`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81c2e3",
   "metadata": {},
   "source": [
    "The `get_completion` function uses the OpenAI API to generate a completion based on a given prompt. Here's a breakdown of how the function works:\n",
    "\n",
    "1. The function takes two parameters: `prompt` (the input text for the model) and `model` (the model to use for generating the completion). By default, the function uses the `gpt-3.5-turbo` model.\n",
    "\n",
    "2. It initializes a `messages` list with a single dictionary element. The dictionary contains two keys: `\"role\"` and `\"content\"`. The role is set to `\"user\"` to indicate that the message is from the user, and the content is set to the provided `prompt`.\n",
    "\n",
    "3. The `openai.ChatCompletion.create()` method is called to generate the completion. It takes several parameters:\n",
    "   - `model`: The model to use for generating the completion.\n",
    "   - `messages`: The list of messages for the conversation. In this case, it contains only the user's prompt.\n",
    "   - `temperature`: This parameter controls the randomness of the model's output. A temperature of `0` means the output will be deterministic and focused, while a higher value like `1.0` introduces more randomness.\n",
    "\n",
    "4. The function retrieves the completion text from the API response. It accesses the first element of the `choices` list in the response and then extracts the content of the message.\n",
    "\n",
    "5. Finally, the function returns the generated completion text.\n",
    "\n",
    "You can use this `get_completion` function to interact with the OpenAI API and generate responses or completions based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921b7e1",
   "metadata": {},
   "source": [
    "# `get_completion_large`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e58f3b",
   "metadata": {},
   "source": [
    "The `get_completion_large` function is designed to facilitate large-scale text completion using the OpenAI API. It takes several parameters: `messages`, `model`, `temperature`, and `max_tokens`.\n",
    "\n",
    "Here's a breakdown of how the function works:\n",
    "\n",
    "1. The `messages` parameter is a list of message objects. Each message object has two properties: `role` and `content`. The `role` can be 'system', 'user', or 'assistant', and the `content` contains the actual text of the message.\n",
    "\n",
    "2. The `model` parameter specifies the language model to use. By default, it is set to \"gpt-3.5-turbo\", which is one of the models provided by OpenAI.\n",
    "\n",
    "3. The `temperature` parameter controls the randomness of the generated text. A higher value like 1.0 makes the output more random, while a lower value like 0.2 makes it more deterministic.\n",
    "\n",
    "4. The `max_tokens` parameter sets the maximum number of tokens in the generated response. Tokens are chunks of text used by the language model for processing.\n",
    "\n",
    "5. The function initializes a `continuation_token` variable to keep track of the continuation of the conversation.\n",
    "\n",
    "6. Inside a while loop, the function makes an API call to `openai.Completion.create` with the provided parameters, including the `messages` list and the `continuation_token`.\n",
    "\n",
    "7. The API response contains a `choices` list, which contains the generated message. The function extracts the `content` from the first choice and adds it to the `messages` list with the 'system' role.\n",
    "\n",
    "8. The `continuation_token` is updated with the finish reason from the API response.\n",
    "\n",
    "9. If the finish reason is 'stop', indicating that the completion process is complete, the loop breaks.\n",
    "\n",
    "10. Finally, the function returns the content of the generated message from the `response.choices[0].message[\"content\"]`.\n",
    "\n",
    "This function can be used to interact with the OpenAI API for large-scale text completion tasks, where the conversation continues until a stop condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436a920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_large(messages, \n",
    "                        model=\"gpt-3.5-turbo\", \n",
    "                        temperature=0, \n",
    "                        max_tokens=1000):\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        response = openai.Completion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature, \n",
    "            max_tokens=max_tokens,\n",
    "            continuation_token=continuation_token\n",
    "        )\n",
    "\n",
    "        chunk = response.choices[0].message['content']\n",
    "        messages.append({'role': 'system', 'content': chunk})\n",
    "\n",
    "        continuation_token = response['choices'][0]['finish_reason']\n",
    "\n",
    "        if continuation_token == 'stop':\n",
    "            break\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c607ce",
   "metadata": {},
   "source": [
    "# `get_completion_from_messages`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616c4c1",
   "metadata": {},
   "source": [
    "The `get_completion_from_messages` function is a custom function that interacts with the OpenAI API Chat Completion endpoint to generate a completion based on a list of messages. Here is a breakdown of how it works:\n",
    "\n",
    "1. The function takes the following parameters:\n",
    "   - `messages`: A list of message objects. Each message object contains a \"role\" (either \"system\", \"user\", or \"assistant\") and \"content\" (the actual text of the message).\n",
    "   - `model`: The model to use for generating the completion. The default value is set to \"gpt-3.5-turbo\", which is a high-performance language model provided by OpenAI.\n",
    "   - `temperature`: The temperature value determines the randomness of the generated output. A higher value like 1.0 makes the output more diverse and creative, while a lower value like 0.2 makes it more focused and deterministic. The default value is set to 0, meaning the output will be deterministic.\n",
    "   - `max_tokens`: The maximum number of tokens allowed in the generated completion. Tokens can be thought of as units of text (e.g., words or characters). The default value is set to 500 tokens.\n",
    "\n",
    "2. The function calls the `openai.ChatCompletion.create()` method, which sends a request to the OpenAI API's Chat Completion endpoint. It passes the required parameters:\n",
    "   - `model`: The model to use for completion, as specified in the function argument.\n",
    "   - `messages`: The list of messages, as specified in the function argument.\n",
    "   - `temperature`: The temperature value, as specified in the function argument.\n",
    "   - `max_tokens`: The maximum number of tokens, as specified in the function argument.\n",
    "\n",
    "3. The response from the API call is stored in the `response` variable.\n",
    "\n",
    "4. The function extracts the content of the completion from the `response` object. It accesses the `choices` attribute of the response, which contains a list of completions. Since we specified `max_tokens=500` and we want only the first completion, it retrieves `response.choices[0]`. The completion's content is accessed using `response.choices[0].message[\"content\"]`.\n",
    "\n",
    "5. Finally, the content of the completion is returned as the output of the function.\n",
    "\n",
    "This function allows you to easily generate completions from a list of messages using the OpenAI API, abstracting away the complexity of making the API call and handling the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87320c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d393c63",
   "metadata": {},
   "source": [
    "# Functions for fundamental analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46beb0",
   "metadata": {},
   "source": [
    "# `preprocess_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2ed25",
   "metadata": {},
   "source": [
    "The function `preprocess_text` is a simple text preprocessing function that performs a series of operations to clean and normalize the input text. Here's a breakdown of how it works:\n",
    "\n",
    "1. Lowercasing the text: The function converts all the characters in the text to lowercase. This is done to ensure consistency and avoid treating the same word as different based on its case. For example, \"Hello\" and \"hello\" will be considered the same after lowercasing.\n",
    "\n",
    "2. Removing special characters: The function uses regular expressions (`re`) to remove any non-alphanumeric characters from the text. Special characters such as punctuation marks, symbols, and other non-alphabetic or non-numeric characters are removed. This step helps in removing noise and focusing on the actual words in the text.\n",
    "\n",
    "3. Replacing multiple spaces: The function replaces multiple consecutive spaces with a single space. This step is useful to handle cases where there are multiple spaces between words, ensuring that words are separated by only one space.\n",
    "\n",
    "The purpose of this preprocessing function is to clean the input text and make it more suitable for further text analysis or natural language processing tasks. By lowercasing the text, removing special characters, and normalizing spaces, the function helps standardize the text data and improve the accuracy of subsequent analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36551e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b829bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_screener_data():\n",
    "\n",
    "    for i in range(0, len(screener_tabs)):\n",
    "        tabname = screener_tabs[i]\n",
    "        f = open(f'{company_data}/{tabname}.txt')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        screener_data[tabname] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153980c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_info_analysis():\n",
    "    file = f'{company_data}/company_info.txt'\n",
    "    data = 'No company info'\n",
    "    if os.path.isfile(file):\n",
    "        f = open(file)\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "    print('Analyzing company_info data...') \n",
    "    system_message = f'As a financial analyst for equity markets, perform an evaluation of the company based on the inputs provided. The input is enclosed within {delimiter}.\\\n",
    "        You must to the analysis in the following steps.\\\n",
    "        Step 8: Prepare a short description of the companys business, factories, plants and operations in general.\\\n",
    "        Step 9: Prepare shareholding trend and status, separetly, if shareholding data is provided. \\\n",
    "        Step 10: Perpare a separate detailed summary of concall data if provided. \\\n",
    "        Step 11: If credit rating data is provided, list out positive and negative points separately. \\\n",
    "        Give your analysis in as detailed manner as possible, however summarize it to limit to max_tokens = 2000 '\n",
    "    user_message = f'{delimiter}{data}{delimiter}'\n",
    "    messages =  [  \n",
    "        {'role':'system', \n",
    "        'content': system_message},    \n",
    "        {'role':'user', \n",
    "        'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "        ] \n",
    "    response = get_completion_from_messages(messages,max_tokens=2000)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6aa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin_statement_analysis():\n",
    "    print('Analyzing screener data...')\n",
    "    system_message = f'As a financial analyst for equity markets, you need to perform an evaluation of the company based on the inputs provided. Some of these inputs will be standard financial data and some will be unstructured. \\\n",
    "            The input data will be encloded with {delimiter} You must to the analysis in the following steps. \\\n",
    "            Step 1:{delimiter} Perform a financial analysis of the company from stock market investing perspective from its annual income statement quarterly income statment \\\n",
    "                balance sheet and cashflow statement. Each will be provided to you enclosed as {delimiter}income_statement:{delimiter} {delimiter}balance_sheet{delimiter} and so on. \\\n",
    "                Step 2: Using the ratio_analysis statement analyze the working capital cycle. Step 3: Perform a Du-Pont analysis using the above data. Step 4: Perform profitibility analysis of this financial data\\\n",
    "                    Step 5: Provide trend analysis and competitive advantages of the company based on given financial data. Step 6: Check pricing power of this company? \\\n",
    "                    Step 6: Detect and report any red flags about the company from the data \\\n",
    "                    Step 7: Report preparation/ Take special care. As an analyst perform these analysis and prepare a report that is very detailed but summarize it to limit to  max_tokens=2000.'\n",
    "\n",
    "    msg = ''\n",
    "    for key,val in screener_data.items():\n",
    "        msg = f'{delimiter}{key}:{val}{delimiter}'\n",
    "    user_message = f'{delimiter}{msg}{delimiter}'\n",
    "    messages =  [  \n",
    "        {'role':'system', \n",
    "        'content': system_message},    \n",
    "        {'role':'user', \n",
    "        'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "        ] \n",
    "    response = ''\n",
    "    response = get_completion_from_messages(messages,max_tokens=2000)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd06479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_pdf(input_file, output_file):\n",
    "    # Create a new PDF object\n",
    "    pdf = FPDF()\n",
    "\n",
    "    # Set the font and font size\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Open the input text file for reading with the correct encoding\n",
    "    with open(input_file, \"r\", encoding=\"latin-1\") as file:\n",
    "        # Read the contents of the file\n",
    "        text = file.read()\n",
    "\n",
    "    # Add a page and set the left/top margins\n",
    "    pdf.add_page()\n",
    "    pdf.set_left_margin(10)\n",
    "    pdf.set_top_margin(10)\n",
    "\n",
    "    # Write the text into the PDF\n",
    "    pdf.multi_cell(0, 10, text)\n",
    "\n",
    "    # Save the PDF to the output file\n",
    "    pdf.output(output_file)\n",
    "\n",
    "def convert_to_four_lists(lst):\n",
    "    result = []\n",
    "    for i in range(0, len(lst), 4):\n",
    "        result.append(lst[i:i+4])\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = pdf_reader.pages\n",
    "        page_count = len(pdf_reader.pages)\n",
    "        l = list(range(2,int(page_count)))\n",
    "        converted_lists = convert_to_four_lists(l)\n",
    "        lo = []\n",
    "        for li in converted_lists:\n",
    "            text = \"\"\n",
    "            for page_num in li:\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "            lo.append(text)\n",
    "    return lo\n",
    "\n",
    "def append_to_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(data)\n",
    "        file.write('\\n') \n",
    "\n",
    "def analyze_concall_data(concall_file_path, analysis_file_path):\n",
    "    extracted_text = extract_text_from_pdf(concall_file_path)\n",
    "    system_message = f'As a financial analyst for equity markets, you need to do the analysis of concall from the given input data. Some of these inputs will be standard financial data and some will be unstructured. \\\n",
    "            The input data will be encloded with {delimiter} You must to the analysis in the following steps. \\\n",
    "            Step 1:{delimiter} Provide the highlights in terms of capex and reinvestment, future growth prospects of growth from the conversation'\n",
    "    for text in extracted_text:\n",
    "        user_message = f'{delimiter}{text}{delimiter}'\n",
    "        messages =  [ {'role':'system',  'content': system_message}, {'role':'user', 'content': f\"{delimiter}{user_message}{delimiter}\"},]\n",
    "        response = get_completion_from_messages(messages,max_tokens=500)\n",
    "        append_to_file(analysis_file_path, response)\n",
    "        print(response)\n",
    "        time.sleep(60)\n",
    "        \n",
    "    return \"File analysed successfully\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd5cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing screener data...\n",
      "Analyzing company_info data...\n",
      "Highlights:\n",
      "- Persistent Systems Limited achieved a revenue of USD 1,035.98 million, giving a growth of 35.3% YoY.\n",
      "- EBIT margin for the full year FY23 was 14.9%, implying 100 basis points improvement YoY.\n",
      "- The Total Contract Value for the quarter came in at USD 421.6 million with new booking TCV coming in at USD 250.3 million, implying a healthy growth in TCV of 16.8% YoY.\n",
      "- The top customer revenue saw a healthy growth of 30.6% Q-on-Q in USD terms, and revenues from the second-largest customer recovered from the furlough impact during the Q4 quarter, registering a sequential growth of 7.6%.\n",
      "- Two additional customers moved into the greater than USD 30 million revenue threshold. \n",
      "\n",
      "There is no specific mention of capex and reinvestment in the given input data. However, the company's growth in revenue, EBIT margin, and order book indicates a positive outlook for future prospects of growth. The signing of a couple of large strategic deals totaling to USD 100 million TCV is also a positive sign for the company's growth potential.\n",
      "Highlights:\n",
      "- Persistent Systems Limited crossed the one billion milestone in revenue for FY23 with Y-o-Y growth of 35.3%.\n",
      "- Services revenue grew by 5.5% quarter-on-quarter.\n",
      "- IP revenue declined by 14.6% due to seasonality in the business.\n",
      "- Healthy revenue growth was seen across most regions, led by the ramp-up of recent deals.\n",
      "- The company added 291 people on a net basis, taking the total employee headcount to 22,889.\n",
      "- Utilization for the quarter came in at 77.3% as against 77.6%.\n",
      "- The company is looking for acquisitions in Consumer Technology, Cybersecurity, Generative AI, European expansion, and some key domains in Financial Services and Healthcare.\n",
      "- The Board of Directors declared a final dividend of INR 12 per share for FY23 and a special dividend of INR 10 per share.\n",
      "- The company is expanding its delivery network to provide best-in-class facilities to its employees in locations close to them.\n",
      "Highlights:\n",
      "- The company had an increase in Purchase Royalty expense in this quarter due to recent large deals and licenses as part of delivery for long-term projects.\n",
      "- The IP-led revenue growth is not directly linked to the growth in the IP or Purchase Royalty item.\n",
      "- EBITDA for the full year grew by 58.5% YoY, and EBITDA margin for FY23 was 18.2% as against 16.8% in FY22.\n",
      "- The company had a lower treasury income in FY23 partly due to lower treasury size post payout for acquisitions done in Q4 of last year and Q1 of this year.\n",
      "- The company had a final dividend of INR 12 per share and a special dividend of INR 10 per share.\n",
      "- The operational capex for the quarter was INR 1,232 million, including increased facilities in India locations.\n",
      "- The company had key deal wins in various industries, including a USD 100 million deal over a five-year period with a Fortune-50 Technology company for its Data Warehouse and Mobile Application Development products.\n",
      "Highlights from the concall of Persistent Systems Limited are as follows:\n",
      "\n",
      "- The company has won several awards and recognitions from industry-leading analyst firms and associations.\n",
      "- The company has delivered healthy growth and profitability in Q4 despite an increasingly difficult macro environment.\n",
      "- The company expects to grow revenues in subsequent quarters anywhere between 3%-5%, 4%-6%.\n",
      "- The company is committed to growing the order book and delivering the best possible growth rates despite project cancellations, ramp downs, and delays in project starts.\n",
      "- The company is not worried about the M&A event announced by the European Union client as the deal construct is fixed for the next five years.\n",
      "- The gross block of software increased by INR 38-39 crores in Q4, and the Purchase and Royalty costs may decline on a Q-o-Q basis going forward.\n",
      "Highlights:\n",
      "- Persistent Systems has made significant investment in cybersecurity area.\n",
      "- Purchase royalty may moderate in percentage terms but will stay in the same range in absolute terms.\n",
      "- No significant ramp downs in other clients except for the hyper scaler account.\n",
      "- Revenue from the large deal is all services, which will see a significant increase and profitability.\n",
      "- Offshoring from Hi-tech customer base is expected to increase due to cost restructuring.\n",
      "- Subcontracting expenses may remain at around 12-12.5% of revenue on a steady state basis.\n",
      "- Wage increments for FY24 may be lower than last year due to inflation moderation.\n",
      "Highlights:\n",
      "- Persistent Systems Limited had a close to 50% revenue growth, but it translated to just 13% growth in operating cash flow due to working capital pressure.\n",
      "- The company faced a structural situation due to the SVB crisis in the last quarter, which affected cash flow.\n",
      "- The company had announced in Q2 and Q3 that increasing DSO was on account of deals in the IP area with large enterprise customers, which have a deeper credit arrangement.\n",
      "- The company is conscious of the working capital blocking and will work towards improving cash flow and bringing down the gap between DSO and working capital.\n",
      "- The company expects the environment to be tough for the next one to two quarters, but there are still opportunities for growth.\n",
      "- The company has invested in forward-looking lines, including developing extensions to existing products and building a practice around Viva, which is integrated with OpenAI.\n",
      "Highlights:\n",
      "- The company is investing in generative AI and Viva components, which are reusable components that will differentiate them in the market.\n",
      "- The company plans to hire 850-1,000 freshers in the next year, and laterals as needed.\n",
      "- Utilization is a lever for the company, and they have a good scope to increase it.\n",
      "- The company is looking to acquire capabilities to take to existing customers and build more revenue.\n",
      "- The company has a defined policy on payout ratios and will focus on a good combination of dividends and tuck-in acquisitions for growth.\n",
      "- The company expects consistent and growing revenue trends, with softness in the first two quarters.\n",
      "From the conversation, it seems that Persistent Systems Limited is not changing its strategy from a pure OPD focus to end-of-lifecycle product investment strategy. However, they may be opportunistic about investing in end-of-lifecycle products if it is profitable and gives them access to enterprise customers. There was no specific mention of capex or reinvestment plans. The future prospects of growth were not discussed in this conversation.\n"
     ]
    }
   ],
   "source": [
    "screener_xls_data = {}\n",
    "delimiter = \"####\"\n",
    "\n",
    "company_data = 'D:/Technofunda Hackathon/Persistent'\n",
    "\n",
    "screener_tabs = ['income_statement','income_statement_quarterly', 'balance_sheet', 'cashflow_statement', 'ratio_analysis','DuPont_Analysis']\n",
    "screener_data = {}\n",
    "\n",
    "load_screener_data()\n",
    "\n",
    "#Financial statement analysis from screener data\n",
    "fin_screener_analysis = ''\n",
    "fin_screener_analysis = fin_statement_analysis()\n",
    "# print(fin_screener_analysis)\n",
    "with open(f'{company_data}/financial_analysis.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(fin_screener_analysis)\n",
    "\n",
    "#Perform company info analysis from data from internet and elsewhere\n",
    "co_info_analysis = ''\n",
    "co_info_analysis = company_info_analysis()\n",
    "with open(f'{company_data}/company_info_analysis.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(co_info_analysis)\n",
    "    \n",
    "concall_file_name = \"PERSISTENT.pdf\"\n",
    "concall_analysis_file_name = \"PERSISTENT_concall_analysis\"\n",
    "time.sleep(60)    \n",
    "#Perform latest concall analysis\n",
    "analyze_concall_data(f'{company_data}/{concall_file_name}',f'{company_data}/{concall_analysis_file_name}.txt')\n",
    "text_to_pdf(f'{company_data}/{concall_analysis_file_name}.txt',f'{company_data}/{concall_analysis_file_name}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
